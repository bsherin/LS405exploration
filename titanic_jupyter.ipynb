{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "djw3mFFEQbRS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFxLWBEmQbRS"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbhnG8ObQbRT"
      },
      "source": [
        "<img src=\"http://www.nltk.org/images/supervised-classification.png\" alt=\"drawing\" style=\"width:400px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oMxB8utQbRT"
      },
      "source": [
        "## The Titanic Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYi31CFYQbRT"
      },
      "source": [
        "Dataset from Kaggle [here](https://www.kaggle.com/c/titanic/overview)\n",
        "\n",
        "> This is the legendary Titanic ML competition  the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n",
        "The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n",
        "\n",
        "We are starting with a non-text dataset since classifying text adds some complexity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkMf6XifQbRT"
      },
      "source": [
        "<table>\n",
        "<tbody>\n",
        "<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n",
        "<tr>\n",
        "<td>survival</td>\n",
        "<td>Survival</td>\n",
        "<td>0 = No, 1 = Yes</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>pclass</td>\n",
        "<td>Ticket class</td>\n",
        "<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>sex</td>\n",
        "<td>Sex</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Age</td>\n",
        "<td>Age in years</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>sibsp</td>\n",
        "<td># of siblings / spouses aboard the Titanic</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>parch</td>\n",
        "<td># of parents / children aboard the Titanic</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>ticket</td>\n",
        "<td>Ticket number</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>fare</td>\n",
        "<td>Passenger fare</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>cabin</td>\n",
        "<td>Cabin number</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>embarked</td>\n",
        "<td>Port of Embarkation</td>\n",
        "<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeT_3fwTQbRU"
      },
      "source": [
        "Read in the corpus. The result here will be a list of dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b3vamIjhQbRU"
      },
      "outputs": [],
      "source": [
        "# file_id = \"1lXO7JEO99fLidhHJDdZUpxinXTiU8Vf-\"\n",
        "# url = f'https://drive.google.com/uc?id={file_id}'\n",
        "url = 'corpora/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "dlist = df.to_dict('records')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT2eFuefQeni"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysVFxqxOQbRU"
      },
      "source": [
        "Shuffle the list of dictionaries. Then split it into two parts, one for training and one for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "piGvOPjnQbRU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(dlist)\n",
        "train_size = int(.9 * len(dlist))\n",
        "train_list = dlist[:train_size]\n",
        "test_list = dlist[train_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcmNtq2JQbRU"
      },
      "source": [
        "### Our First Classifier\n",
        "We just guess, for every person, that they died. So, in this case, there's no training to be done.\n",
        "\n",
        "We'll create two lists, a `gold_list` and a `guess_list`. The `gold_list` has the right answers. The `guess_list` has the guess made by our classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZAB1tAKQbRU"
      },
      "outputs": [],
      "source": [
        "gold_list = [r[\"Survived\"] for r in test_list]\n",
        "guess_list = [0 for r in test_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtRfWvrHQbRU"
      },
      "source": [
        "nltk has a tool for drawing a confusion matrix, given the gold_list and guess_list.\n",
        "\n",
        "We'll also look at how many were correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVFY8b0VQbRV"
      },
      "outputs": [],
      "source": [
        "def pprint(txt):\n",
        "    print(\"<pre>{}</pre>\".format(txt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpwP5moKQbRV"
      },
      "outputs": [],
      "source": [
        "cm = nltk.ConfusionMatrix(gold_list, guess_list)\n",
        "pprint(cm)\n",
        "accuracy = (cm[0, 0] + cm[1, 1]) / len (test_list)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M70esvWfQbRV"
      },
      "source": [
        "#### Cohen's Kappa\n",
        "We should use another measure here, Cohen's Kappa, that adjusts for agreement by chance.\n",
        "\n",
        "```math\n",
        "\\large \\kappa=\\frac{p_o-p_e}{1-p_e}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYweBC1XQbRV"
      },
      "source": [
        "where `$p_0$` is the relative observed agreement among raters, and `$p_e$` is the hypothetical probability of chance agreement, using the observed data to calculate the probabilities of each observer randomly seeing each category. If the raters are in complete agreement then `$\\kappa=1$`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ5MdFPnQbRV"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "cohen_kappa_score(gold_list, guess_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueuFa8bgQbRV"
      },
      "source": [
        "## Our second classifier: All with Sex=Female survive\n",
        "\n",
        "Now let's try guessing that everyone female survived and every male dies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYr_M5GQQbRV"
      },
      "outputs": [],
      "source": [
        "def simple_classifier(r):\n",
        "    if r[\"Sex\"] == \"female\":\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMWnliEHQbRV"
      },
      "outputs": [],
      "source": [
        "random.shuffle(dlist)\n",
        "train_list = dlist[:train_size]\n",
        "test_list = dlist[train_size:]\n",
        "gold_list = [r[\"Survived\"] for r in test_list]\n",
        "guess_list = [simple_classifier(r) for r in test_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFmQTwOHQbRV"
      },
      "outputs": [],
      "source": [
        "cm = nltk.ConfusionMatrix(gold_list, guess_list)\n",
        "pprint(cm)\n",
        "accuracy = (cm[0, 0] + cm[1, 1]) / len (test_list)\n",
        "print(\"accuracy = \" + str(accuracy))\n",
        "\"kappa = \" + str(cohen_kappa_score(gold_list, guess_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4vjLbpmQbRV"
      },
      "source": [
        "This works pretty well. In a way, that's a problem, since it will be hard to do better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBCfdbiHQbRV"
      },
      "source": [
        "## Set up a more formal structure\n",
        "\n",
        "We want a more formal structure, both to organize our work, and because nltk expect data structures with a particular form.\n",
        "\n",
        "We don't really need ot start over. But we will so we have everything in one place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI3oGtQiQbRV"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('corpora/titanic.csv')\n",
        "dlist = df.to_dict('records')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxGoRC6LQbRV"
      },
      "source": [
        "We convert the data to **labeled feature sets**. Each \"feature set\" is a dictionary, with the features, plus the plus the \"label\" which is the right answer for that feature set.\n",
        "\n",
        "We'll start with an extremely simple feature set, which just includes the \"Sex\" variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vpswff3QbRV"
      },
      "outputs": [],
      "source": [
        "def passenger_features(r):\n",
        "    return {\"sex\": r[\"Sex\"]}\n",
        "\n",
        "labeled_feature_sets = [(passenger_features(r), r[\"Survived\"]) for r in dlist]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pyq1UOoQbRV"
      },
      "source": [
        "We split the labeled feature sets into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVzmfbxkQbRV"
      },
      "outputs": [],
      "source": [
        "train_size = int(.9 * len(dlist))\n",
        "\n",
        "train_set = labeled_feature_sets[:train_size]\n",
        "test_set = labeled_feature_sets[train_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UWGrAiFQbRV"
      },
      "source": [
        "**Create and train a classifier**\n",
        "\n",
        "There are many machine learning algorithms that we can use.\n",
        "From a practical point of view, switching from one algorithm to another can be as easy as changing part of one line of code. But you might want to change other parts of the pipeline.\n",
        "\n",
        "We're going to start by making use of an algorithm called **Naive Bayes**, in part because it's relatively easy to explain how it works. (I'm going to explain it in a bit.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRx50IOaQbRV"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "titanic_classifier = nltk.NaiveBayesClassifier.train(train_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7lPkjf0QbRW"
      },
      "source": [
        "We can now use this classifier to classify individual *feature sets*. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLyMImkvQbRW"
      },
      "outputs": [],
      "source": [
        "print(labeled_feature_sets[0])\n",
        "titanic_classifier.classify(labeled_feature_sets[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddhslwDOQbRW"
      },
      "outputs": [],
      "source": [
        "print(labeled_feature_sets[1])\n",
        "titanic_classifier.classify(labeled_feature_sets[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bLZpzH5QbRW"
      },
      "source": [
        "**Evaluate**: Look at how this classifier does overall on various metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74k9MIFKQbRW"
      },
      "outputs": [],
      "source": [
        "gold_list = [t[1] for t in test_set]\n",
        "guess_list = [titanic_classifier.classify(t[0]) for t in test_set]\n",
        "cm = nltk.ConfusionMatrix(gold_list, guess_list)\n",
        "pprint(cm)\n",
        "accuracy = nltk.classify.accuracy(titanic_classifier, test_set)\n",
        "print(\"accuracy = \" + str(accuracy))\n",
        "print(\"kappa = \" + str(cohen_kappa_score(gold_list, guess_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0yEbqYBQbRW"
      },
      "source": [
        "**Altogether**: Let's put it all together now, but with a more extensive feature set.\n",
        "\n",
        "This cell runs our whole process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqG5Er12QbRW"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('corpora/titanic.csv')\n",
        "dlist = df.to_dict('records')\n",
        "random.shuffle(dlist)\n",
        "\n",
        "def passenger_features(r):\n",
        "    return {\"sex\": r[\"Sex\"], \"pclass\": r[\"Pclass\"], \"embarked\": r[\"Embarked\"]}\n",
        "\n",
        "labeled_feature_sets = [(passenger_features(r), r[\"Survived\"]) for r in dlist]\n",
        "train_set = labeled_feature_sets[:train_size]\n",
        "test_set = labeled_feature_sets[train_size:]\n",
        "titanic_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "gold_list = [t[1] for t in test_set]\n",
        "guess_list = [titanic_classifier.classify(t[0]) for t in test_set]\n",
        "cm = nltk.ConfusionMatrix(gold_list, guess_list)\n",
        "pprint(cm)\n",
        "accuracy = nltk.classify.accuracy(titanic_classifier, test_set)\n",
        "print(\"accuracy = \" + str(accuracy))\n",
        "print(\"kappa = \" + str(cohen_kappa_score(gold_list, guess_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYDOVyp5QbRW"
      },
      "source": [
        "It's not clear whether this is better or worse than using the feature set that only had \"sex.\"\n",
        "\n",
        "The naive bayes classifier in nltk will also show us the \"most informative features.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI6fM-q8QbRW"
      },
      "outputs": [],
      "source": [
        "def show_most_informative_features(classif, number=25):\n",
        "    print(\"<pre>\")\n",
        "    classif.show_most_informative_features(25)\n",
        "    print(\"</pre>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0QMQctLQbRW"
      },
      "outputs": [],
      "source": [
        "show_most_informative_features(titanic_classifier, 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe197PvHQbRX"
      },
      "source": [
        "## How Naive Bayes works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bisGtku7QbRX"
      },
      "source": [
        "### The problem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxUzEzCbQbRX"
      },
      "source": [
        "We want to know the probability of a given label given a set of features.\n",
        "\n",
        "```math\n",
        "\\large P(L|f_1 + f_2 + f_3+...)\n",
        "```\n",
        "\n",
        "For example, in the Titanic case, we'd like to know the probability that a passenger died, given that they were female, were in class 2, and embarked at S. If we knew the probability that they died, and the probability that they lived, we could pick the one that was larger.\n",
        "\n",
        "We can use the training data to estimate probabilities like this.\n",
        "\n",
        "Using the training set, we can easily gather data about how often a feature occurs\n",
        "with a given label\n",
        "\n",
        "```math\n",
        "\\large P(f_1|L), P(f_2|L), P(f_3|L)\n",
        "```\n",
        "\n",
        "So, the question is, can we get to the former from the latter?\n",
        "\n",
        "The answer is yes, with Bayes Theorem and a bit of fudging.\n",
        "\n",
        "```math\n",
        "\\large P(L|f_1 + f_2 + f_3+...) = \\frac{P(L)P(f_1|L)P(f_2|L)P(f_3|L)}{P(f_1)P(f_2)P(f_3)}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC2IPPrvQbRX"
      },
      "source": [
        "Bayes theorem will let us flip the L and fs.\n",
        "Being \"naive\" will let us split up the fs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jE1oxvwhQbRX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ls405_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
